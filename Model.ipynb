{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from keras.preprocessing import image\n",
    "from sklearn.cluster import DBSCAN\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Input\n",
    "import h5py\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from time import time\n",
    "from images_functions import print_photos_for_every_label\n",
    "from helper_functions import get_batches\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from pca_functions import fitting_pca, pca_transform\n",
    "import itertools\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(\"reduced_result.csv\")\n",
    "\n",
    "#Delete NaNs\n",
    "nans = pd.isnull(train_data).any(1).nonzero()[0]\n",
    "train_data = train_data.drop(train_data.index[list(nans)])\n",
    "\n",
    "#Getting labels in binary form\n",
    "labels_temp = [l.split() for l in train_data[\"labels\"]]\n",
    "labels = np.array(list(map(lambda x:list(map(lambda y:int(y), x)), labels_temp)))\n",
    "MB = MultiLabelBinarizer()\n",
    "Y_train = MB.fit_transform(labels)\n",
    "\n",
    "X_train = np.array([i for i in train_data[\"features\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dirs with photos\n",
    "TRAIN_DIR = config.TRAIN_DIR\n",
    "#TEST_DIR = \"D:/test_photos/\"\n",
    "\n",
    "#CSV files with photos ids and corresponding business ids \n",
    "#test_photo_to_biz = pd.read_csv(\"D:/test_photo_to_biz.csv\")\n",
    "train_photo_to_biz = config.train_photo_to_biz\n",
    "\n",
    "#CSV file with labels corresponding to business\n",
    "lbs_pd = config.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches, lbs = get_batches(train_photo_to_biz, lbs_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = IncrementalPCA(n_components=1712, batch_size = 1712)\n",
    "f1 = h5py.File(\"data1.h5\", \"r\")\n",
    "datasets1 = sorted([i for i in f1], key=lambda x:int(x[5:]))\n",
    "f2 = h5py.File(\"data2.h5\", \"r\")\n",
    "datasets2 = sorted([i for i in f2], key=lambda x:int(x[5:]))\n",
    "f3 = h5py.File(\"data3.h5\", \"r\")\n",
    "datasets3 = sorted([i for i in f3], key=lambda x:int(x[5:]))\n",
    "temp_data = 0\n",
    "for i in tqdm(range(len(datasets1))):\n",
    "    temp_data = fitting_pca(pca, f1, datasets1[i], temp_data, pca.n_components)\n",
    "for i in tqdm(range(len(datasets2))):\n",
    "    temp_data = fitting_pca(pca, f2, datasets2[i], temp_data, pca.n_components)\n",
    "for i in tqdm(range(len(datasets3))):\n",
    "    temp_data = fitting_pca(pca, f3, datasets3[i], temp_data, pca.n_components)\n",
    "        \n",
    "result_file = h5py.File(\"result_pca1712.h5\", \"w\")\n",
    "\n",
    "for i in tqdm(range(len(datasets1))):\n",
    "    pca_transform(pca, f1, datasets1[i], 5, result_file)\n",
    "for i in tqdm(range(len(datasets2))):\n",
    "    pca_transform(pca, f2, datasets2[i], 5, result_file)\n",
    "for i in tqdm(range(len(datasets3))):\n",
    "    pca_transform(pca, f3, datasets3[i], 5, result_file)\n",
    "    \n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "\n",
    "\n",
    "result_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File(\"model4_output.h5\", \"r\")\n",
    "businesses = file[\"businesses\"][:]\n",
    "imagenet_labels = file[\"labels\"][:]\n",
    "file.close()\n",
    "lbs = imagenet_labels.reshape(234842, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "n = 0\n",
    "for i in np.unique(businesses):\n",
    "    if type(list(lbs_pd[lbs_pd[\"business_id\"]==i][\"labels\"])[0])!=float:\n",
    "        Y.append(list(map(lambda x:int(x), list(lbs_pd[lbs_pd[\"business_id\"]==i][\"labels\"])[0].split())))\n",
    "        X.append(np.sum(lbs[np.where(businesses==i)], axis=0))\n",
    "        n+=1\n",
    "        print(n)\n",
    "Y = MultiLabelBinarizer().fit_transform(Y)\n",
    "X_imgnet = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_ids = [i[0] for i in batches]\n",
    "for i in range(len(businesses)):\n",
    "    if businesses[i] not in bs_ids:\n",
    "        lbs = np.delete(lbs,i,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"D:/dataset3/result_pca1712.h5\"\n",
    "file = h5py.File(Path, \"r\")\n",
    "datasets = sorted([i for i in file], key=lambda x:int(x[5:]))\n",
    "data = file[datasets[0]][:]\n",
    "for i in range(1,len(datasets)):\n",
    "    data = np.concatenate([data, file[datasets[i]][:]], axis=0)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"kmeans_imgnet.h5\",\"r\")\n",
    "kmeans_imgnet_labels = f[\"labels\"][:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"kmeans1712.h5\",\"r\")\n",
    "kmeans_labels = f[\"labels\"][:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_cluster(kmeans_labels, data, num):\n",
    "    lbs_per_smpl = [[[] for j in range(8)] for i in range(len(batches))]\n",
    "    n = 0\n",
    "    for i in range(len(batches)):\n",
    "        for j in range(len(batches[i][1])):\n",
    "            lbs_per_smpl[i][kmeans_labels[n]].append(data[n])\n",
    "            n+=1\n",
    "    mean_per_lbl = [0 for i in range(len(batches))]\n",
    "    for i in range(len(lbs_per_smpl)):\n",
    "        if lbs_per_smpl[i]!=[]:\n",
    "            mean_per_lbl[i] = list(map(lambda x:np.mean(x, axis=0) if x!=[] else 0, lbs_per_smpl[i]))\n",
    "        else:\n",
    "            mean_per_lbl[i] = []\n",
    "        \n",
    "    for i in range(len(mean_per_lbl)):\n",
    "        n = 0\n",
    "        for j in range(len(mean_per_lbl[i])):\n",
    "            if type(mean_per_lbl[i][j-n])==int:\n",
    "                del mean_per_lbl[i][j-n]\n",
    "                n+=1\n",
    "            \n",
    "    X_cls = [0 for i in range(len(mean_per_lbl))]\n",
    "    for i in range(len(mean_per_lbl)):\n",
    "        X_cls[i] = np.concatenate(mean_per_lbl[i])\n",
    "    \n",
    "    for i in range(len(X_cls)):\n",
    "        while(X_cls[i].shape[0]<num):\n",
    "            X_cls[i] = np.concatenate([X_cls[i],X_cls[i]])\n",
    "        X_cls[i] = X_cls[i][:num]\n",
    "    \n",
    "    X_cls = np.array(X_cls)\n",
    "    return X_cls\n",
    "X_cls = group_cluster(kmeans_labels, data, 13696)\n",
    "X_imgnet_cls = group_cluster(kmeans_imgnet_labels, lbs, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_classifier = MLPClassifier(solver='lbfgs', max_iter=300, \n",
    "                               hidden_layer_sizes=(1024,512,128,32,16,))\n",
    "\n",
    "xgb_classifier = OneVsRestClassifier(XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"))\n",
    "\n",
    "svm_classifier = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True))\n",
    "\n",
    "mlp_classifier_clst = MLPClassifier(solver='lbfgs', max_iter=300, hidden_layer_sizes=(1024,512,128,32,16,))\n",
    "\n",
    "xgb_classifier_clst = OneVsRestClassifier(XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"))\n",
    "\n",
    "svm_classifier_clst = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True))\n",
    "\n",
    "mlp_ensemble = MLPClassifier(solver='sgd', max_iter=5, hidden_layer_sizes=(74,44,))\n",
    "\n",
    "cls_chain1 = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[0,6,2,1,5,7,4,8,3])\n",
    "\n",
    "cls_chain2 = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[1,2,6,5,8,4,7,0,3])\n",
    "\n",
    "cls_chain3 = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[7,2,6,5,3,4,1,8,0])\n",
    "\n",
    "cls_chain4 = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[4,2,1,5,3,7,8,6,0])\n",
    "\n",
    "cls_chain5 = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[5,2,7,1,6,8,4,0,3])\n",
    "\n",
    "cls_chain1_clst = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[0,6,2,1,5,7,4,8,3])\n",
    "\n",
    "cls_chain2_clst = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[1,2,6,5,8,4,7,0,3])\n",
    "\n",
    "cls_chain3_clst = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[7,2,6,5,3,4,1,8,0])\n",
    "\n",
    "cls_chain4_clst = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[4,2,1,5,3,7,8,6,0])\n",
    "\n",
    "cls_chain5_clst = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[5,2,7,1,6,8,4,0,3])\n",
    "\n",
    "mlp_classifier_img = MLPClassifier(solver='lbfgs', max_iter=300, hidden_layer_sizes=(1024,512,128,32,16,))\n",
    "\n",
    "xgb_classifier_img = OneVsRestClassifier(XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"))\n",
    "\n",
    "svm_classifier_img = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True))\n",
    "\n",
    "mlp_classifier_clst_img = MLPClassifier(solver='lbfgs', max_iter=300, hidden_layer_sizes=(1024,512,128,32,16,))\n",
    "\n",
    "xgb_classifier_clst_img = OneVsRestClassifier(XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"))\n",
    "\n",
    "svm_classifier_clst_img = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True))\n",
    "\n",
    "mlp_ensemble_img = MLPClassifier(solver='sgd', max_iter=5, hidden_layer_sizes=(74,44,))\n",
    "\n",
    "cls_chain1_img = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[0,6,2,1,5,7,4,8,3])\n",
    "\n",
    "cls_chain2_img = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[1,2,6,5,8,4,7,0,3])\n",
    "\n",
    "cls_chain3_img = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[7,2,6,5,3,4,1,8,0])\n",
    "\n",
    "cls_chain4_img = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[4,2,1,5,3,7,8,6,0])\n",
    "\n",
    "cls_chain5_img = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[5,2,7,1,6,8,4,0,3])\n",
    "\n",
    "cls_chain1_clst_img = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[0,6,2,1,5,7,4,8,3])\n",
    "\n",
    "cls_chain2_clst_img = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[1,2,6,5,8,4,7,0,3])\n",
    "\n",
    "cls_chain3_clst_img = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[7,2,6,5,3,4,1,8,0])\n",
    "\n",
    "cls_chain4_clst_img = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[4,2,1,5,3,7,8,6,0])\n",
    "\n",
    "cls_chain5_clst_img = ClassifierChain(base_estimator=XGBClassifier(num_class=9, gamma=0.024, learning_rate=0.3, max_depth=6,\n",
    "                                                   n_estimators=100, objective=\"multi:softmax\"), order=[5,2,7,1,6,8,4,0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(classifier, X_train, Y_train):\n",
    "    \n",
    "    results = []\n",
    "    label_results = [] #Results for each label separately\n",
    "    predictions = []\n",
    "    \n",
    "    beg = 1900\n",
    "    end = 1996\n",
    "    \n",
    "    #Testing on each part of data set with re-training\n",
    "    for i in tqdm(range(1)):\n",
    "        \n",
    "        x_train = np.concatenate([X_train[:beg],X_train[end:]], axis=0)\n",
    "        y_train = np.concatenate([Y_train[:beg],Y_train[end:]], axis=0)\n",
    "        \n",
    "        x_test = X_train[beg:end]\n",
    "        y_test = Y_train[beg:end]\n",
    "    \n",
    "        classifier.fit(x_train, y_train)\n",
    "    \n",
    "        y_pred = classifier.predict(x_test)\n",
    "        \n",
    "        predictions.append(y_pred)\n",
    "        \n",
    "        results.append(f1_score(y_test, y_pred, average=\"micro\"))\n",
    "    \n",
    "        label_results.append([f1_score(list(map(lambda x:x[i], y_test)),list(map(lambda x:x[i], y_pred)), average=\"micro\") for i in range(9)])\n",
    "    \n",
    "        end = beg\n",
    "        beg-=100\n",
    "    \n",
    "        classifier = clone(classifier)\n",
    "    return results, label_results, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [mlp_classifier, mlp_classifier_clst, xgb_classifier, xgb_classifier_clst, svm_classifier, svm_classifier_clst,\n",
    "               cls_chain1, cls_chain1_clst, cls_chain2, cls_chain2_clst, cls_chain3, cls_chain3_clst, cls_chain4, \n",
    "               cls_chain4_clst, cls_chain5, cls_chain5_clst,\n",
    "              mlp_classifier_img, mlp_classifier_clst_img, xgb_classifier_img, xgb_classifier_clst_img, svm_classifier_img, svm_classifier_clst_img,\n",
    "               cls_chain1_img, cls_chain1_clst_img, cls_chain2_img, cls_chain2_clst_img, cls_chain3_img, cls_chain3_clst_img, cls_chain4_img, \n",
    "               cls_chain4_clst_img, cls_chain5_img, cls_chain5_clst_img]\n",
    "\n",
    "\n",
    "classifiers_str = [\"mlp_classifier\", \"mlp_classifier_clst\", \"xgb_classifier\", \"xgb_classifier_clst\", \"svm_classifier\", \n",
    "                   \"svm_classifier_clst\", \"cls_chain1\", \"cls_chain1_clst\", \"cls_chain2\", \"cls_chain2_clst\", \"cls_chain3\", \n",
    "                   \"cls_chain3_clst\", \"cls_chain4\", \"cls_chain4_clst\", \"cls_chain5\", \"cls_chain5_clst\",\n",
    "                  \"mlp_classifier_img\", \"mlp_classifier_clst_img\", \"xgb_classifier_img\", \"xgb_classifier_clst_img\", \"svm_classifier_img\", \n",
    "                   \"svm_classifier_clst_img\", \"cls_chain1_img\", \"cls_chain1_clst_img\", \"cls_chain2_img\", \"cls_chain2_clst_img\", \"cls_chain3_img\", \n",
    "                   \"cls_chain3_clst_img\", \"cls_chain4_img\", \"cls_chain4_clst_img\", \"cls_chain5_img\", \"cls_chain5_clst_img\"]\n",
    "\n",
    "\n",
    "f = h5py.File(\"classifiers_result.h5\", \"w\")\n",
    "\n",
    "\n",
    "labels = [\"good_for_lunch\",\"good_for_dinner\",\"takes_reservations\",\"outdoor_seating\",\"restaurant_is_expensive\",\"has_alcohol\",\n",
    "          \"has_table_service\",\"ambience_is_classy\",\"good_for_kids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results\n",
    "def print_results(label_results):\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    mean_label_result = np.mean(label_results, axis=0)\n",
    "\n",
    "    for i in range(9):\n",
    "        \n",
    "        print(labels[i]+\" : \"+str(mean_label_result[i])+\"\\n\")\n",
    "\n",
    "    print(\"Total: \"+str(np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16,len(classifiers)):\n",
    "    \n",
    "    if \"clst_img\" in classifiers_str[i]:\n",
    "        \n",
    "        results, label_results, predictions = make_predictions(classifiers[i], X_imgnet_cls, Y)\n",
    "    \n",
    "    elif \"clst\" in classifiers_str[i]:\n",
    "        \n",
    "        results, label_results, predictions = make_predictions(classifiers[i], X_cls, Y_train)\n",
    "    \n",
    "    elif \"img\" in classifiers_str[i]:\n",
    "        \n",
    "        results, label_results, predictions = make_predictions(classifiers[i], X_imgnet, Y)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        results, label_results, predictions = make_predictions(classifiers[i], X_train, Y_train)\n",
    "    \n",
    "    preds = np.concatenate(predictions, axis=0)\n",
    "    \n",
    "    lbs_res = np.concatenate(label_results, axis=0)\n",
    "    \n",
    "    print(f\"\\n{classifiers_str[i]}\\n\")\n",
    "    \n",
    "    print_results(label_results)\n",
    "    \n",
    "    f.create_dataset(f\"{classifiers_str[i]}_preds\", data=preds)\n",
    "    \n",
    "    f.create_dataset(f\"{classifiers_str[i]}_lbs\", data=lbs_res)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_photos_for_every_label(labels, predictions[0], Y_train[1900:], batches, TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation between labels\")\n",
    "\n",
    "corr = list(map(lambda x:list(map(lambda y:str(np.round(y,2)) ,x)) ,list(np.corrcoef(Y_train.T))))\n",
    "\n",
    "for i in range(9):\n",
    "    \n",
    "    print(labels[i], i)\n",
    "\n",
    "print(\"    \"+\"        \".join([str(i) for i in range(9)]))\n",
    "\n",
    "for i in range(9):\n",
    "    \n",
    "    print(i, list(map(lambda x:x+(\"00\" if len(x)<4 else \"0\" if len(x)<5 else \"\"),corr[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"good_for_lunch\",\"good_for_dinner\",\"takes_reservations\",\"outdoor_seating\",\"restaurant_is_expensive\",\"has_alcohol\",\"has_table_service\",\"ambience_is_classy\",\"good_for_kids\"]\n",
    "\n",
    "print(\"Percentage of each lable:\\n\")\n",
    "\n",
    "for i in range(9):\n",
    "    \n",
    "    print(labels[i]+\" \"*(25-len(labels[i])), np.round(np.count_nonzero(Y_train.T[i])/1996, 3))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of each pair of labels\")\n",
    "\n",
    "for i in range(9):\n",
    "    \n",
    "    print(\"\\n\"+labels[i], i)\n",
    "\n",
    "pair_matrix = [[0 for i in range(10)] for j in range(10)]\n",
    "\n",
    "for i in range(1996):\n",
    "    \n",
    "    indxs = np.where(Y_train[i]==1)[0]\n",
    "    \n",
    "    pairs = list(itertools.combinations(sorted(indxs), 2))\n",
    "    \n",
    "    for i in pairs:\n",
    "        \n",
    "        pair_matrix[i[0]][i[1]]+=1\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    for j in range(10):\n",
    "        \n",
    "        if pair_matrix[i][j]!=0:\n",
    "            \n",
    "            print(f\"\\n{i} and {j} : \", np.round(pair_matrix[i][j]/1996, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
