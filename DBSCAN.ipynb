{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helper_functions import read_photos, embeddings_to_file, get_batches\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Flatten, Input\n",
    "from keras.models import Model\n",
    "import h5py\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps = 480, min_samples=2, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dirs with photos\n",
    "TRAIN_DIR = config.TRAIN_DIR\n",
    "#TEST_DIR = \"D:/test_photos/\"\n",
    "\n",
    "#CSV files with photos ids and corresponding business ids \n",
    "#test_photo_to_biz = pd.read_csv(\"D:/test_photo_to_biz.csv\")\n",
    "train_photo_to_biz = config.train_photo_to_biz\n",
    "\n",
    "#CSV file with labels corresponding to business\n",
    "lbs_pd = config.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0831 12:43:40.308590  9180 deprecation_wrapper.py:119] From C:\\Users\\vaitovich\\AppData\\Local\\Continuum\\anaconda3\\envs\\data-science\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0831 12:43:40.359821  9180 deprecation_wrapper.py:119] From C:\\Users\\vaitovich\\AppData\\Local\\Continuum\\anaconda3\\envs\\data-science\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0831 12:43:40.379079  9180 deprecation_wrapper.py:119] From C:\\Users\\vaitovich\\AppData\\Local\\Continuum\\anaconda3\\envs\\data-science\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0831 12:43:40.500344  9180 deprecation_wrapper.py:119] From C:\\Users\\vaitovich\\AppData\\Local\\Continuum\\anaconda3\\envs\\data-science\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0831 12:43:40.500344  9180 deprecation_wrapper.py:119] From C:\\Users\\vaitovich\\AppData\\Local\\Continuum\\anaconda3\\envs\\data-science\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0831 12:43:46.837394  9180 deprecation_wrapper.py:119] From C:\\Users\\vaitovich\\AppData\\Local\\Continuum\\anaconda3\\envs\\data-science\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0831 12:43:47.238261  9180 deprecation_wrapper.py:119] From C:\\Users\\vaitovich\\AppData\\Local\\Continuum\\anaconda3\\envs\\data-science\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "C:\\Users\\vaitovich\\AppData\\Local\\Continuum\\anaconda3\\envs\\data-science\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "ResNet_model = ResNet50(weights='imagenet', include_top = False)\n",
    "input1 = Input(shape=(224, 224, 3), name='image_input')\n",
    "features = ResNet_model(input1)\n",
    "features = Flatten()(features)\n",
    "features_model = Model(inputs=input1, outputs=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches, lbs = get_batches(train_photo_to_biz, lbs_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_to_file(\"embeddings\", TRAIN_DIR, batches, features_model, 2000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"embeddings.h5\", \"r\")\n",
    "datasets = sorted([i for i in f], key=lambda x:int(x[5:]))\n",
    "embeddings = f[datasets[0]][:]\n",
    "for i in range(1,len(datasets)):\n",
    "    embeddings = np.concatenate((embeddings, f[datasets[i]][:]), axis=0)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dists = euclidean_distances(embeddings, embeddings)\n",
    "sorted_emb_dists = np.sort(emb_dists)\n",
    "dists_to_plot = sorted([np.sum(vec[:2])/2 for vec in sorted_emb_dists])\n",
    "plt.plot(dists_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"D:/result_pca.h5\"\n",
    "file = h5py.File(Path, \"r\")\n",
    "datasets = sorted([i for i in file], key=lambda x:int(x[5:]))\n",
    "data100 = file[datasets[0]][:]\n",
    "for i in range(1,len(datasets)):\n",
    "    data100 = np.concatenate([data100, file[datasets[i]][:]], axis=0)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.fit(data100)\n",
    "dbscan_lbs = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"dbscan_labels.h5\", \"w\")\n",
    "f.create_dataset(\"lbs\", data=dbscan_lbs)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# photos = []\n",
    "# photos_id = []\n",
    "# photos_to_plot = []\n",
    "# for i in range(len(batches)):\n",
    "#         photos_to_plot+=read_photos(batches[i][1], True)\n",
    "#         photos_id+=list(batches[i][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
