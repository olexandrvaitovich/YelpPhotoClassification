{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Flatten, Input, Dense, Lambda\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from processing import merge_pickles, data_to_pickle\n",
    "from helper_functions import get_filenames\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using pretrained model as backbone\n",
    "VGG_model = VGG16(weights='imagenet', include_top = False)\n",
    "# #Adding layers\n",
    "input_layer = Input(shape=(224, 224, 3), name='image_input')\n",
    "conv_out = VGG_model(input_layer)\n",
    "flattened = Flatten()(conv_out)\n",
    "\n",
    "# #Model for extracting features from images\n",
    "features_model = Model(inputs=input_layer, outputs=flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dirs with photos\n",
    "TRAIN_DIR = config.TRAIN_DIR\n",
    "#TEST_DIR = \"D:/test_photos/\"\n",
    "\n",
    "DATA_DIR = config.DATA_DIR\n",
    "\n",
    "#CSV files with photos ids and corresponding business ids \n",
    "#test_photo_to_biz = pd.read_csv(\"D:/test_photo_to_biz.csv\")\n",
    "train_photo_to_biz = config.train_photo_to_biz\n",
    "\n",
    "#CSV file with labels corresponding to business\n",
    "labels = config.labels\n",
    "max_img_amount = config.max_img_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting train features\n",
    "\n",
    "X_train = []\n",
    "\n",
    "for i in tqdm(range(len(train_photo_to_biz))):\n",
    "    \n",
    "    #Reading images corresponding to photo_to_biz DataFrame\n",
    "    img_path = TRAIN_DIR + str(train_photo_to_biz.photo_id[i]) + '.jpg'\n",
    "    img = np.expand_dims(image.img_to_array(image.load_img(img_path, target_size=(224, 224))), axis=0)\n",
    "    \n",
    "    #Processing and extracting features from images\n",
    "    processed_img = preprocess_input(img)\n",
    "    X_train.append(features_model.predict(processed_img))\n",
    "    \n",
    "    #Deliting images to free RAM\n",
    "    del processed_img\n",
    "    del img\n",
    "    del img_path\n",
    "    \n",
    "    #Writing images to pickles to free RAM\n",
    "    #Number of i to enter this block is maximum number of images could be holded in RAM \n",
    "    #10000=8GB RAM\n",
    "    \n",
    "    if i%max_img_amount==0 and i!=0:\n",
    "        \n",
    "        data_to_pickle(X_train, train_photo_to_biz, labels, i-max_img_amount, i, True)\n",
    "        \n",
    "        del X_train\n",
    "        \n",
    "        X_train = []\n",
    "\n",
    "data_to_pickle(X_train, train_photo_to_biz, labels, len(train_photo_to_biz)-(len(train_photo_to_biz)%max_img_amount), len(train_photo_to_biz), True)\n",
    "\n",
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting test features\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for i in tqdm(range(len(test_photo_to_biz))):\n",
    "    \n",
    "    #Reading images corresponding to photo_to_biz DataFrame\n",
    "    img_path = TEST_DIR + str(test_photo_to_biz.photo_id[i]) + '.jpg'\n",
    "    img = np.expand_dims(image.img_to_array(image.load_img(img_path, target_size=(224, 224))), axis=0)\n",
    "    \n",
    "    #Processing and extracting features from images\n",
    "    processed_img = preprocess_input(img)\n",
    "    X_test.append(features_model.predict(processed_img))\n",
    "    \n",
    "    #Deliting images to free RAM\n",
    "    del processed_img\n",
    "    del img\n",
    "    del img_path\n",
    "    \n",
    "    #Writing images to pickles to free RAM\n",
    "    #Number of i to enter this block is maximum number of images could be holded in RAM \n",
    "    #10000=8GB RAM\n",
    "    \n",
    "    if i%max_img_amount==0 and i!=0:\n",
    "        \n",
    "        data_to_pickle(X_test, test_photo_to_biz, None, i-max_img_amount, i, False)\n",
    "        \n",
    "        del X_test\n",
    "        \n",
    "        X_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting pickled data\n",
    "\n",
    "data_files = get_filenames(DATA_DIR)\n",
    "\n",
    "filtered_data_files = list(filter(lambda y:\"train_data\" in y, list(filter(lambda x:x[-3:]==\"csv\", data_files))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging pickled data\n",
    "main_df = pd.read_pickle(filtered_data_files[0])\n",
    "\n",
    "for i in range(1,len(filtered_data_files)):\n",
    "    \n",
    "    main_df = merge_pickles(filtered_data_files[i], main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking mean vector for every business\n",
    "grouped_df = pd.DataFrame(main_df.groupby(\"business_id\")[\"features\"].apply(np.mean))\n",
    "\n",
    "grouped_df.reset_index(level=0, inplace=True)\n",
    "\n",
    "lb = labels[labels[\"business_id\"].isin(grouped_df[\"business_id\"])]\n",
    "\n",
    "sorted_labels = lb.sort_values(\"business_id\")\n",
    "\n",
    "sorted_labels.reset_index(level=0,drop=True, inplace=True)\n",
    "\n",
    "grouped_df[\"labels\"] = sorted_labels[\"labels\"]\n",
    "\n",
    "del sorted_labels\n",
    "\n",
    "#Clearing\n",
    "\n",
    "nans = pd.isnull(grouped_df).any(1).nonzero()[0]\n",
    "\n",
    "cleared_df = grouped_df.drop(grouped_df.index[list(nans)])\n",
    "\n",
    "cleared_df.to_pickle(\"embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimentionality reduction\n",
    "df = pd.read_pickle(\"result_df.csv\")\n",
    "\n",
    "arr = np.array(list(map(lambda x:x.squeeze(),df[\"features\"])))\n",
    "\n",
    "pca = PCA(.999)\n",
    "\n",
    "pca.fit(arr)\n",
    "\n",
    "print(pca.n_components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pca.transform(arr)\n",
    "\n",
    "pd_df=pd.DataFrame({\"features\":list(new_df), \"labels\":df[\"labels\"], \"business_id\":df[\"business_id\"]})\n",
    "pd_df.to_pickle(\"reduced_result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
